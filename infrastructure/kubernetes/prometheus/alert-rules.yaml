# Prometheus Alerting Rules
# SLO-based alerts focusing on user-impacting issues
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alert-rules
  namespace: observability
  labels:
    app: prometheus
    component: monitoring
data:
  alert-rules.yml: |
    groups:
      # SLO-based alerts for service availability
      - name: slo_availability
        interval: 30s
        rules:
          # Service availability < 99.9% over 5m (0.1% error budget burn)
          - alert: HighErrorRate
            expr: |
              (
                sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
                /
                sum(rate(http_requests_total[5m])) by (service)
              ) > 0.001
            for: 5m
            labels:
              severity: critical
              category: slo
            annotations:
              summary: "High error rate for {{ $labels.service }}"
              description: "{{ $labels.service }} has {{ $value | humanizePercentage }} error rate (threshold: 0.1%)"
              runbook_url: "https://docs.health-tracker.example.com/runbooks/high-error-rate"

          # Service completely down (no requests)
          - alert: ServiceDown
            expr: |
              (
                sum(rate(http_requests_total[5m])) by (service) == 0
                and
                sum(rate(http_requests_total[15m] offset 5m)) by (service) > 0
              )
            for: 5m
            labels:
              severity: critical
              category: slo
            annotations:
              summary: "{{ $labels.service }} is completely down"
              description: "No requests received for {{ $labels.service }} in last 5 minutes"

      # SLO-based alerts for latency
      - name: slo_latency
        interval: 30s
        rules:
          # P95 latency > 1s (SLO breach)
          - alert: HighLatency
            expr: |
              histogram_quantile(0.95,
                sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
              ) > 1
            for: 10m
            labels:
              severity: warning
              category: slo
            annotations:
              summary: "High P95 latency for {{ $labels.service }}"
              description: "{{ $labels.service }} P95 latency is {{ $value }}s (SLO: <1s)"

          # P99 latency > 5s (critical SLO breach)
          - alert: CriticalLatency
            expr: |
              histogram_quantile(0.99,
                sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
              ) > 5
            for: 5m
            labels:
              severity: critical
              category: slo
            annotations:
              summary: "Critical P99 latency for {{ $labels.service }}"
              description: "{{ $labels.service }} P99 latency is {{ $value }}s (SLO: <5s)"

      # Infrastructure health alerts
      - name: infrastructure_health
        interval: 1m
        rules:
          # Node CPU usage > 80%
          - alert: NodeHighCPU
            expr: |
              (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance)) * 100 > 80
            for: 15m
            labels:
              severity: warning
              scope: instance
            annotations:
              summary: "High CPU usage on {{ $labels.instance }}"
              description: "Node {{ $labels.instance }} CPU usage is {{ $value | humanize }}%"

          # Node memory usage > 85%
          - alert: NodeHighMemory
            expr: |
              (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
            for: 10m
            labels:
              severity: warning
              scope: instance
            annotations:
              summary: "High memory usage on {{ $labels.instance }}"
              description: "Node {{ $labels.instance }} memory usage is {{ $value | humanize }}%"

          # Node disk usage > 85%
          - alert: NodeHighDiskUsage
            expr: |
              (1 - (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"})) * 100 > 85
            for: 30m
            labels:
              severity: warning
              scope: instance
            annotations:
              summary: "High disk usage on {{ $labels.instance }}"
              description: "{{ $labels.instance }}:{{ $labels.mountpoint }} disk usage is {{ $value | humanize }}%"

          # Critical disk usage > 95%
          - alert: NodeCriticalDiskUsage
            expr: |
              (1 - (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"})) * 100 > 95
            for: 10m
            labels:
              severity: critical
              scope: instance
            annotations:
              summary: "Critical disk usage on {{ $labels.instance }}"
              description: "{{ $labels.instance }}:{{ $labels.mountpoint }} disk usage is {{ $value | humanize }}%"

      # Kubernetes cluster health
      - name: kubernetes_health
        interval: 1m
        rules:
          # Pod crash looping
          - alert: PodCrashLooping
            expr: |
              rate(kube_pod_container_status_restarts_total[15m]) > 0.05
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
              description: "Pod has restarted {{ $value | humanize }} times in last 15 minutes"

          # Pod pending too long
          - alert: PodPendingTooLong
            expr: |
              kube_pod_status_phase{phase="Pending"} == 1
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} pending too long"
              description: "Pod has been in Pending state for >15 minutes"

          # Deployment replicas mismatch
          - alert: DeploymentReplicasMismatch
            expr: |
              kube_deployment_spec_replicas != kube_deployment_status_replicas_available
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has mismatched replicas"
              description: "Expected {{ $value }} replicas but only {{ $labels.available }} are available"

      # OTel Collector health
      - name: otel_collector_health
        interval: 30s
        rules:
          # Data loss - dropped spans
          - alert: OTelCollectorDroppingSpans
            expr: |
              rate(otelcol_processor_dropped_spans[5m]) > 0
            for: 10m
            labels:
              severity: critical
              team: platform
            annotations:
              summary: "OTel Collector dropping spans"
              description: "Collector is dropping {{ $value | humanize }} spans/sec"

          # Export failures
          - alert: OTelCollectorExportFailures
            expr: |
              rate(otelcol_exporter_send_failed_spans[5m]) > 10
            for: 10m
            labels:
              severity: warning
              team: platform
            annotations:
              summary: "OTel Collector export failures"
              description: "Collector failing to export {{ $value | humanize }} spans/sec"

          # Queue building up
          - alert: OTelCollectorQueueFull
            expr: |
              otelcol_exporter_queue_size > 1000
            for: 15m
            labels:
              severity: warning
              team: platform
            annotations:
              summary: "OTel Collector queue building up"
              description: "Exporter queue size is {{ $value }}"

      # Prometheus health
      - name: prometheus_health
        interval: 1m
        rules:
          # TSDB compaction failing
          - alert: PrometheusCompactionFailing
            expr: |
              rate(prometheus_tsdb_compactions_failed_total[1h]) > 0
            for: 15m
            labels:
              severity: warning
              team: platform
            annotations:
              summary: "Prometheus TSDB compaction failing"
              description: "Compaction failures: {{ $value | humanize }}/hour"

          # High rule evaluation time
          - alert: PrometheusRuleEvaluationSlow
            expr: |
              prometheus_rule_group_last_duration_seconds > 60
            for: 15m
            labels:
              severity: warning
              team: platform
            annotations:
              summary: "Prometheus rule evaluation taking too long"
              description: "Rule group {{ $labels.rule_group }} taking {{ $value }}s (should be <60s)"

          # Target down
          - alert: PrometheusTargetDown
            expr: |
              up == 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Prometheus target {{ $labels.job }}/{{ $labels.instance }} down"
              description: "Target has been unreachable for >5 minutes"

          # Storage usage high
          - alert: PrometheusStorageHigh
            expr: |
              (
                prometheus_tsdb_storage_blocks_bytes
                /
                (prometheus_tsdb_retention_limit_bytes > 0)
              ) > 0.85
            for: 30m
            labels:
              severity: warning
              team: platform
            annotations:
              summary: "Prometheus storage usage high"
              description: "Storage is {{ $value | humanizePercentage }} full"

      # Thanos sidecar health
      - name: thanos_health
        interval: 1m
        rules:
          # Upload failures
          - alert: ThanosUploadFailing
            expr: |
              rate(thanos_objstore_bucket_operation_failures_total{operation="upload"}[5m]) > 0
            for: 15m
            labels:
              severity: warning
              team: platform
            annotations:
              summary: "Thanos failing to upload blocks to S3"
              description: "Upload failure rate: {{ $value | humanize }}/sec"

          # Upload lag
          - alert: ThanosUploadLag
            expr: |
              (time() - thanos_objstore_bucket_last_successful_upload_time) > 7200
            for: 15m
            labels:
              severity: warning
              team: platform
            annotations:
              summary: "Thanos hasn't uploaded blocks recently"
              description: "Last upload was {{ $value | humanizeDuration }} ago (should be <2h)"

      # Database health
      - name: database_health
        interval: 1m
        rules:
          # PostgreSQL connections high
          - alert: PostgreSQLConnectionsHigh
            expr: |
              (
                sum(pg_stat_database_numbackends) by (instance)
                /
                pg_settings_max_connections
              ) > 0.8
            for: 10m
            labels:
              severity: warning
              team: dba
              service: postgres
            annotations:
              summary: "PostgreSQL connections high on {{ $labels.instance }}"
              description: "Using {{ $value | humanizePercentage }} of max connections"

          # Redis memory high
          - alert: RedisMemoryHigh
            expr: |
              (
                redis_memory_used_bytes
                /
                redis_memory_max_bytes
              ) > 0.9
            for: 15m
            labels:
              severity: warning
              team: dba
              service: redis
            annotations:
              summary: "Redis memory usage high on {{ $labels.instance }}"
              description: "Using {{ $value | humanizePercentage }} of max memory"

      # CI/CD pipeline health
      - name: cicd_health
        interval: 5m
        rules:
          # Build failure rate high
          - alert: HighBuildFailureRate
            expr: |
              (
                sum(rate(github_workflow_run_conclusion_total{conclusion="failure"}[1h])) by (workflow)
                /
                sum(rate(github_workflow_run_conclusion_total[1h])) by (workflow)
              ) > 0.2
            for: 30m
            labels:
              severity: warning
              team: platform
            annotations:
              summary: "High build failure rate for {{ $labels.workflow }}"
              description: "{{ $value | humanizePercentage }} of builds failing in last hour"

          # Build duration too long
          - alert: BuildDurationTooLong
            expr: |
              histogram_quantile(0.95,
                sum(rate(github_workflow_run_duration_seconds_bucket[1h])) by (workflow, le)
              ) > 1800
            for: 1h
            labels:
              severity: info
              team: platform
            annotations:
              summary: "Build duration for {{ $labels.workflow }} is too long"
              description: "P95 build time is {{ $value | humanizeDuration }} (should be <30m)"
