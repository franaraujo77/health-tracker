name: Test Scenario - All Validations Pass

# This workflow tests the scenario where all validation checks pass successfully
# It verifies that the orchestrator correctly aggregates results and triggers Claude review

on:
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run mode (skip Claude review trigger)'
        required: false
        type: boolean
        default: true

permissions:
  contents: read
  checks: write
  pull-requests: write

jobs:
  # Use the test orchestrator with all-pass scenario
  run-test:
    name: Execute All-Pass Test
    uses: ./.github/workflows/test-orchestrator.yml
    with:
      test_scenario: 'all-pass'
      enable_assertions: true

  # Verify orchestrator behavior
  verify-orchestrator-outputs:
    name: Verify Orchestrator Outputs
    runs-on: ubuntu-latest
    needs: run-test
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test results
        uses: actions/download-artifact@v4
        with:
          name: test-results-${{ needs.run-test.outputs.workflow-run-id }}
          path: test-results/
        continue-on-error: true

      - name: Verify all-passed output
        id: verify-all-passed
        run: |
          # Expected: all-passed should be true
          EXPECTED="true"
          ACTUAL="${{ needs.run-test.outputs.all-passed }}"

          echo "## Verification: all-passed Output" >> $GITHUB_STEP_SUMMARY
          echo "- Expected: \`${EXPECTED}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Actual: \`${ACTUAL}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$ACTUAL" = "$EXPECTED" ]; then
            echo "✅ **PASS**: all-passed output is correct" >> $GITHUB_STEP_SUMMARY
            echo "result=PASS" >> $GITHUB_OUTPUT
          else
            echo "❌ **FAIL**: all-passed output is incorrect" >> $GITHUB_STEP_SUMMARY
            echo "result=FAIL" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Verify has-critical-failures output
        id: verify-critical-failures
        run: |
          # Expected: has-critical-failures should be false
          EXPECTED="false"
          ACTUAL="${{ needs.run-test.outputs.has-critical-failures }}"

          echo "## Verification: has-critical-failures Output" >> $GITHUB_STEP_SUMMARY
          echo "- Expected: \`${EXPECTED}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Actual: \`${ACTUAL}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$ACTUAL" = "$EXPECTED" ]; then
            echo "✅ **PASS**: has-critical-failures output is correct" >> $GITHUB_STEP_SUMMARY
            echo "result=PASS" >> $GITHUB_OUTPUT
          else
            echo "❌ **FAIL**: has-critical-failures output is incorrect" >> $GITHUB_STEP_SUMMARY
            echo "result=FAIL" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Verify error report structure
        id: verify-error-report
        run: |
          if [ ! -f "test-results/error-report.json" ]; then
            echo "❌ **FAIL**: error-report.json not found" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

          echo "## Verification: Error Report Structure" >> $GITHUB_STEP_SUMMARY

          # Validate JSON structure
          if ! jq empty test-results/error-report.json 2>/dev/null; then
            echo "❌ **FAIL**: error-report.json is not valid JSON" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

          # Check required fields
          REQUIRED_FIELDS=("timestamp" "workflow_run" "test_scenario" "all_passed" "has_critical_failures" "errors")
          ALL_PRESENT=true

          for field in "${REQUIRED_FIELDS[@]}"; do
            if ! jq -e ".$field" test-results/error-report.json > /dev/null 2>&1; then
              echo "❌ **FAIL**: Missing required field: \`${field}\`" >> $GITHUB_STEP_SUMMARY
              ALL_PRESENT=false
            else
              echo "✅ Field present: \`${field}\`" >> $GITHUB_STEP_SUMMARY
            fi
          done

          if [ "$ALL_PRESENT" = "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "✅ **PASS**: All required fields present in error report" >> $GITHUB_STEP_SUMMARY
            echo "result=PASS" >> $GITHUB_OUTPUT
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "❌ **FAIL**: Some required fields missing" >> $GITHUB_STEP_SUMMARY
            echo "result=FAIL" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Verify error array is empty
        id: verify-no-errors
        run: |
          ERROR_COUNT=$(jq '.errors | length' test-results/error-report.json)

          echo "## Verification: Error Array" >> $GITHUB_STEP_SUMMARY
          echo "- Expected error count: \`0\`" >> $GITHUB_STEP_SUMMARY
          echo "- Actual error count: \`${ERROR_COUNT}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$ERROR_COUNT" -eq 0 ]; then
            echo "✅ **PASS**: No errors in error array (as expected for all-pass scenario)" >> $GITHUB_STEP_SUMMARY
            echo "result=PASS" >> $GITHUB_OUTPUT
          else
            echo "❌ **FAIL**: Unexpected errors found in error array" >> $GITHUB_STEP_SUMMARY
            echo "**Errors:**" >> $GITHUB_STEP_SUMMARY
            jq -r '.errors[] | "- [\(.severity)] \(.message)"' test-results/error-report.json >> $GITHUB_STEP_SUMMARY
            echo "result=FAIL" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Verify JSON values match boolean outputs
        id: verify-json-consistency
        run: |
          JSON_ALL_PASSED=$(jq -r '.all_passed' test-results/error-report.json)
          JSON_HAS_CRITICAL=$(jq -r '.has_critical_failures' test-results/error-report.json)

          OUTPUT_ALL_PASSED="${{ needs.run-test.outputs.all-passed }}"
          OUTPUT_HAS_CRITICAL="${{ needs.run-test.outputs.has-critical-failures }}"

          echo "## Verification: JSON vs Outputs Consistency" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          CONSISTENT=true

          if [ "$JSON_ALL_PASSED" != "$OUTPUT_ALL_PASSED" ]; then
            echo "❌ **MISMATCH**: all_passed - JSON: \`${JSON_ALL_PASSED}\`, Output: \`${OUTPUT_ALL_PASSED}\`" >> $GITHUB_STEP_SUMMARY
            CONSISTENT=false
          else
            echo "✅ all_passed consistent: \`${JSON_ALL_PASSED}\`" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "$JSON_HAS_CRITICAL" != "$OUTPUT_HAS_CRITICAL" ]; then
            echo "❌ **MISMATCH**: has_critical_failures - JSON: \`${JSON_HAS_CRITICAL}\`, Output: \`${OUTPUT_HAS_CRITICAL}\`" >> $GITHUB_STEP_SUMMARY
            CONSISTENT=false
          else
            echo "✅ has_critical_failures consistent: \`${JSON_HAS_CRITICAL}\`" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$CONSISTENT" = "true" ]; then
            echo "✅ **PASS**: JSON and workflow outputs are consistent" >> $GITHUB_STEP_SUMMARY
            echo "result=PASS" >> $GITHUB_OUTPUT
          else
            echo "❌ **FAIL**: Inconsistency detected between JSON and outputs" >> $GITHUB_STEP_SUMMARY
            echo "result=FAIL" >> $GITHUB_OUTPUT
            exit 1
          fi

  # Simulate Claude review decision logic
  simulate-claude-review-decision:
    name: Simulate Claude Review Decision
    runs-on: ubuntu-latest
    needs: run-test
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Evaluate Claude review trigger condition
        id: evaluate
        run: |
          ALL_PASSED="${{ needs.run-test.outputs.all-passed }}"
          HAS_CRITICAL="${{ needs.run-test.outputs.has-critical-failures }}"

          echo "## Claude Review Decision Logic" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Inputs:**" >> $GITHUB_STEP_SUMMARY
          echo "- all_passed: \`${ALL_PASSED}\`" >> $GITHUB_STEP_SUMMARY
          echo "- has_critical_failures: \`${HAS_CRITICAL}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Decision logic from claude-review-conditional.yml
          if [ "${ALL_PASSED}" = "true" ]; then
            SHOULD_REVIEW="true"
            DECISION="✅ TRIGGER Claude Code Review"
            REASON="All validations passed successfully"
          else
            SHOULD_REVIEW="false"
            if [ "${HAS_CRITICAL}" = "true" ]; then
              DECISION="❌ SKIP Claude Code Review"
              REASON="Critical validation failures must be fixed first"
            else
              DECISION="⚠️ SKIP Claude Code Review"
              REASON="Non-critical validation issues should be addressed first"
            fi
          fi

          echo "should_review=${SHOULD_REVIEW}" >> $GITHUB_OUTPUT
          echo "decision=${DECISION}" >> $GITHUB_OUTPUT

          echo "**Decision:** ${DECISION}" >> $GITHUB_STEP_SUMMARY
          echo "**Reason:** ${REASON}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

      - name: Verify expected decision for all-pass scenario
        run: |
          EXPECTED_DECISION="true"
          ACTUAL_DECISION="${{ steps.evaluate.outputs.should_review }}"

          echo "## Verification: Claude Review Decision" >> $GITHUB_STEP_SUMMARY
          echo "- Expected should_review: \`${EXPECTED_DECISION}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Actual should_review: \`${ACTUAL_DECISION}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$ACTUAL_DECISION" = "$EXPECTED_DECISION" ]; then
            echo "✅ **PASS**: Claude review would be TRIGGERED (correct for all-pass scenario)" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **FAIL**: Claude review decision is incorrect for all-pass scenario" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: Mock Claude review trigger (dry run)
        if: inputs.dry_run && steps.evaluate.outputs.should_review == 'true'
        run: |
          echo "## Mock Claude Review Trigger" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "🤖 **DRY RUN MODE**: Claude review would be triggered here" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**What would happen:**" >> $GITHUB_STEP_SUMMARY
          echo "1. Set commit status to 'pending'" >> $GITHUB_STEP_SUMMARY
          echo "2. Checkout repository at PR head SHA" >> $GITHUB_STEP_SUMMARY
          echo "3. Get list of changed files" >> $GITHUB_STEP_SUMMARY
          echo "4. Run Claude Code review with retry logic (3 attempts)" >> $GITHUB_STEP_SUMMARY
          echo "5. Post review comments to PR" >> $GITHUB_STEP_SUMMARY
          echo "6. Set commit status to 'success' or 'failure'" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ Test validation complete - all assertions passed" >> $GITHUB_STEP_SUMMARY

  # Final test report
  generate-final-report:
    name: Generate Final Test Report
    runs-on: ubuntu-latest
    needs: [run-test, verify-orchestrator-outputs, simulate-claude-review-decision]
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Generate comprehensive report
        run: |
          echo "# Test Scenario Report: All Validations Pass" > final-report.md
          echo "" >> final-report.md
          echo "**Test Run ID:** ${{ github.run_id }}" >> final-report.md
          echo "**Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> final-report.md
          echo "**Dry Run:** ${{ inputs.dry_run }}" >> final-report.md
          echo "" >> final-report.md

          echo "## Test Execution Summary" >> final-report.md
          echo "" >> final-report.md
          echo "### Orchestrator Test" >> final-report.md
          echo "- **Status:** ${{ needs.run-test.result }}" >> final-report.md
          echo "- **All Passed:** ${{ needs.run-test.outputs.all-passed }}" >> final-report.md
          echo "- **Has Critical Failures:** ${{ needs.run-test.outputs.has-critical-failures }}" >> final-report.md
          echo "" >> final-report.md

          echo "### Verification Tests" >> final-report.md
          echo "- **Orchestrator Outputs:** ${{ needs.verify-orchestrator-outputs.result }}" >> final-report.md
          echo "- **Claude Review Decision:** ${{ needs.simulate-claude-review-decision.result }}" >> final-report.md
          echo "" >> final-report.md

          echo "## Test Results" >> final-report.md
          echo "" >> final-report.md

          # Determine overall test result
          if [ "${{ needs.run-test.result }}" = "success" ] && \
             [ "${{ needs.verify-orchestrator-outputs.result }}" = "success" ] && \
             [ "${{ needs.simulate-claude-review-decision.result }}" = "success" ]; then
            echo "### ✅ Overall Result: PASSED" >> final-report.md
            echo "" >> final-report.md
            echo "All test verifications completed successfully:" >> final-report.md
            echo "- ✅ Mock validations all passed as expected" >> final-report.md
            echo "- ✅ Orchestrator correctly set all-passed=true" >> final-report.md
            echo "- ✅ Orchestrator correctly set has-critical-failures=false" >> final-report.md
            echo "- ✅ Error report structure validated" >> final-report.md
            echo "- ✅ Error array is empty (no errors)" >> final-report.md
            echo "- ✅ Claude review decision logic verified (would trigger)" >> final-report.md
          else
            echo "### ❌ Overall Result: FAILED" >> final-report.md
            echo "" >> final-report.md
            echo "Some test verifications failed. See job details above." >> final-report.md
            echo "" >> final-report.md
            echo "**Job Results:**" >> final-report.md
            echo "- run-test: ${{ needs.run-test.result }}" >> final-report.md
            echo "- verify-orchestrator-outputs: ${{ needs.verify-orchestrator-outputs.result }}" >> final-report.md
            echo "- simulate-claude-review-decision: ${{ needs.simulate-claude-review-decision.result }}" >> final-report.md
          fi

          cat final-report.md >> $GITHUB_STEP_SUMMARY

      - name: Upload final report
        uses: actions/upload-artifact@v4
        with:
          name: all-pass-test-report-${{ github.run_id }}
          path: final-report.md
          retention-days: 30

      - name: Fail workflow if any test failed
        if: needs.run-test.result != 'success' || needs.verify-orchestrator-outputs.result != 'success' || needs.simulate-claude-review-decision.result != 'success'
        run: |
          echo "::error::One or more test verifications failed"
          exit 1
