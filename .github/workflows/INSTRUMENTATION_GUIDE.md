# GitHub Actions Workflow Instrumentation Guide

## Overview

This guide provides complete implementation instructions for instrumenting GitHub Actions workflows with OpenTelemetry telemetry. It covers the remaining tasks from Story 3: **"[backend] Instrument GitHub Actions Workflows with OpenTelemetry"**.

## Completed Work

‚úÖ **Task 1**: Created reusable OTel setup action (`.github/actions/setup-telemetry`)
‚úÖ **Task 2**: Instrumented validation-orchestrator workflow

## Remaining Tasks

### Task 3: Instrument frontend-ci and backend-ci Workflows

#### frontend-ci.yml Instrumentation

**Location**: `.github/workflows/frontend-ci.yml`

**Changes Required**:

1. **Add telemetry setup at the beginning of the workflow**:

```yaml
jobs:
  setup-telemetry:
    name: Setup Telemetry
    runs-on: ubuntu-latest
    outputs:
      trace-id: ${{ steps.telemetry.outputs.trace-id }}
      service-name: ${{ steps.telemetry.outputs.service-name }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        id: telemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'frontend-ci'

      - name: Record workflow start
        run: |
          source $OTEL_HELPERS_PATH
          otel_span_step "frontend-ci-started" "success"
          otel_metric "frontend.workflow.started" 1 "counter"
```

2. **Update each job to depend on setup-telemetry**:

```yaml
  lint:
    name: Lint
    needs: setup-telemetry
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'frontend-lint'

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: |
          source $OTEL_HELPERS_PATH
          otel_exec "npm-ci" npm ci

      - name: Run ESLint
        run: |
          source $OTEL_HELPERS_PATH
          START_TIME=$(date +%s)

          if otel_exec "eslint-check" npm run lint; then
            LINT_STATUS="success"
          else
            LINT_STATUS="failure"
          fi

          DURATION=$(($(date +%s) - START_TIME))
          otel_metric "frontend.lint.duration_seconds" $DURATION "gauge"
          otel_metric "frontend.lint.$LINT_STATUS" 1 "counter"

          [ "$LINT_STATUS" = "success" ] || exit 1
```

3. **Instrument type-check job**:

```yaml
  type-check:
    name: Type Check
    needs: setup-telemetry
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'frontend-typecheck'

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: |
          source $OTEL_HELPERS_PATH
          otel_exec "npm-ci" npm ci

      - name: Run TypeScript compiler
        run: |
          source $OTEL_HELPERS_PATH
          START_TIME=$(date +%s)

          if otel_exec "typescript-check" npm run type-check; then
            TYPE_STATUS="success"
          else
            TYPE_STATUS="failure"
          fi

          DURATION=$(($(date +%s) - START_TIME))
          otel_metric "frontend.typecheck.duration_seconds" $DURATION "gauge"
          otel_metric "frontend.typecheck.$TYPE_STATUS" 1 "counter"

          [ "$TYPE_STATUS" = "success" ] || exit 1
```

4. **Instrument test job with coverage metrics**:

```yaml
  test:
    name: Test
    needs: setup-telemetry
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'frontend-test'

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: |
          source $OTEL_HELPERS_PATH
          otel_exec "npm-ci" npm ci

      - name: Run tests with coverage
        run: |
          source $OTEL_HELPERS_PATH
          START_TIME=$(date +%s)

          if otel_exec "vitest-run" npm run test:coverage -- --run; then
            TEST_STATUS="success"
          else
            TEST_STATUS="failure"
          fi

          DURATION=$(($(date +%s) - START_TIME))

          # Extract coverage metrics
          if [ -f coverage/coverage-summary.json ]; then
            COVERAGE_PCT=$(jq '.total.lines.pct' coverage/coverage-summary.json)
            TESTS_PASSED=$(jq '.numPassedTests // 0' coverage/test-results.json 2>/dev/null || echo "0")
            TESTS_FAILED=$(jq '.numFailedTests // 0' coverage/test-results.json 2>/dev/null || echo "0")

            otel_metric "frontend.test.coverage_percent" "$COVERAGE_PCT" "gauge"
            otel_metric "frontend.test.passed" "$TESTS_PASSED" "counter"
            otel_metric "frontend.test.failed" "$TESTS_FAILED" "counter"
          fi

          otel_metric "frontend.test.duration_seconds" $DURATION "gauge"
          otel_metric "frontend.test.$TEST_STATUS" 1 "counter"

          [ "$TEST_STATUS" = "success" ] || exit 1
```

5. **Instrument build job with bundle size**:

```yaml
  build:
    name: Build
    needs: [lint, type-check, test]
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'frontend-build'

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: |
          source $OTEL_HELPERS_PATH
          otel_exec "npm-ci" npm ci

      - name: Build application
        run: |
          source $OTEL_HELPERS_PATH
          START_TIME=$(date +%s)

          if otel_exec "vite-build" npm run build; then
            BUILD_STATUS="success"
          else
            BUILD_STATUS="failure"
          fi

          DURATION=$(($(date +%s) - START_TIME))

          # Measure bundle size
          if [ -d dist ]; then
            BUNDLE_SIZE=$(du -sb dist | cut -f1)
            otel_metric "frontend.build.bundle_size_bytes" "$BUNDLE_SIZE" "gauge"
          fi

          otel_metric "frontend.build.duration_seconds" $DURATION "gauge"
          otel_metric "frontend.build.$BUILD_STATUS" 1 "counter"

          [ "$BUILD_STATUS" = "success" ] || exit 1
```

#### backend-ci.yml Instrumentation

**Location**: `.github/workflows/backend-ci.yml`

**Similar Pattern**:

1. **Add setup-telemetry job**
2. **Instrument compile job**:

```yaml
  compile:
    name: Compile
    needs: setup-telemetry
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'backend-compile'

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'microsoft'
          java-version: '25'

      - name: Compile with Maven
        run: |
          source $OTEL_HELPERS_PATH
          START_TIME=$(date +%s)

          if otel_exec "mvn-compile" env JAVA_HOME=/Library/Java/JavaVirtualMachines/microsoft-25.jdk/Contents/Home mvn clean compile -B -q; then
            COMPILE_STATUS="success"
          else
            COMPILE_STATUS="failure"
          fi

          DURATION=$(($(date +%s) - START_TIME))
          otel_metric "backend.compile.duration_seconds" $DURATION "gauge"
          otel_metric "backend.compile.$COMPILE_STATUS" 1 "counter"

          [ "$COMPILE_STATUS" = "success" ] || exit 1
```

3. **Instrument unit-test job**:

```yaml
  unit-test:
    name: Unit Tests
    needs: compile
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'backend-unit-test'

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'microsoft'
          java-version: '25'

      - name: Run unit tests
        run: |
          source $OTEL_HELPERS_PATH
          START_TIME=$(date +%s)

          if otel_exec "mvn-test" env JAVA_HOME=/Library/Java/JavaVirtualMachines/microsoft-25.jdk/Contents/Home mvn test -B; then
            TEST_STATUS="success"
          else
            TEST_STATUS="failure"
          fi

          DURATION=$(($(date +%s) - START_TIME))

          # Extract test metrics from Surefire reports
          if [ -d target/surefire-reports ]; then
            TESTS_RUN=$(find target/surefire-reports -name "*.xml" -exec grep -h 'tests=' {} \; | grep -oP 'tests="\K[0-9]+' | awk '{s+=$1} END {print s}')
            TESTS_FAILED=$(find target/surefire-reports -name "*.xml" -exec grep -h 'failures=' {} \; | grep -oP 'failures="\K[0-9]+' | awk '{s+=$1} END {print s}')

            otel_metric "backend.test.run" "${TESTS_RUN:-0}" "counter"
            otel_metric "backend.test.failed" "${TESTS_FAILED:-0}" "counter"
          fi

          otel_metric "backend.test.duration_seconds" $DURATION "gauge"
          otel_metric "backend.test.$TEST_STATUS" 1 "counter"

          [ "$TEST_STATUS" = "success" ] || exit 1
```

4. **Instrument integration-test job** (similar pattern with database service check)

5. **Instrument package job**:

```yaml
  package:
    name: Package
    needs: [unit-test, integration-test]
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'backend-package'

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'microsoft'
          java-version: '25'

      - name: Package application
        run: |
          source $OTEL_HELPERS_PATH
          START_TIME=$(date +%s)

          if otel_exec "mvn-package" env JAVA_HOME=/Library/Java/JavaVirtualMachines/microsoft-25.jdk/Contents/Home mvn package -DskipTests -B; then
            PACKAGE_STATUS="success"
          else
            PACKAGE_STATUS="failure"
          fi

          DURATION=$(($(date +%s) - START_TIME))

          # Measure artifact size
          if [ -f target/*.jar ]; then
            ARTIFACT_SIZE=$(du -sb target/*.jar | cut -f1 | head -1)
            otel_metric "backend.package.artifact_size_bytes" "$ARTIFACT_SIZE" "gauge"
          fi

          otel_metric "backend.package.duration_seconds" $DURATION "gauge"
          otel_metric "backend.package.$PACKAGE_STATUS" 1 "counter"

          [ "$PACKAGE_STATUS" = "success" ] || exit 1
```

### Task 4: Instrument Security and Lighthouse Workflows

#### security-validation.yml Instrumentation

**Location**: `.github/workflows/security-validation.yml`

```yaml
jobs:
  setup-telemetry:
    name: Setup Telemetry
    runs-on: ubuntu-latest
    outputs:
      trace-id: ${{ steps.telemetry.outputs.trace-id }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        id: telemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'security-validation'

  dependency-scan:
    name: Dependency Scan
    needs: setup-telemetry
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'dependency-scan'

      - name: Run dependency check
        run: |
          source $OTEL_HELPERS_PATH
          START_TIME=$(date +%s)

          # Run npm audit for frontend
          if otel_exec "npm-audit" npm audit --audit-level=high; then
            SCAN_STATUS="success"
            VULNERABILITIES=0
          else
            SCAN_STATUS="warning"
            VULNERABILITIES=$(npm audit --json | jq '.metadata.vulnerabilities.high + .metadata.vulnerabilities.critical')
          fi

          DURATION=$(($(date +%s) - START_TIME))
          otel_metric "security.dependency.duration_seconds" $DURATION "gauge"
          otel_metric "security.dependency.vulnerabilities" "$VULNERABILITIES" "gauge"
          otel_metric "security.dependency.$SCAN_STATUS" 1 "counter"
```

#### lighthouse-ci.yml Instrumentation

**Location**: `.github/workflows/lighthouse-ci.yml` (if exists) or create it

```yaml
jobs:
  lighthouse:
    name: Lighthouse Performance
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenTelemetry
        uses: ./.github/actions/setup-telemetry
        with:
          service-name: 'lighthouse-ci'

      - name: Run Lighthouse CI
        run: |
          source $OTEL_HELPERS_PATH
          START_TIME=$(date +%s)

          # Run lighthouse
          if otel_exec "lighthouse-run" npx lighthouse-ci autorun; then
            LH_STATUS="success"
          else
            LH_STATUS="failure"
          fi

          DURATION=$(($(date +%s) - START_TIME))

          # Extract performance metrics
          if [ -f .lighthouseci/manifest.json ]; then
            PERFORMANCE=$(jq '.[0].summary.performance' .lighthouseci/manifest.json)
            ACCESSIBILITY=$(jq '.[0].summary.accessibility' .lighthouseci/manifest.json)
            SEO=$(jq '.[0].summary.seo' .lighthouseci/manifest.json)

            otel_metric "lighthouse.performance_score" "$PERFORMANCE" "gauge"
            otel_metric "lighthouse.accessibility_score" "$ACCESSIBILITY" "gauge"
            otel_metric "lighthouse.seo_score" "$SEO" "gauge"
          fi

          otel_metric "lighthouse.duration_seconds" $DURATION "gauge"
          otel_metric "lighthouse.$LH_STATUS" 1 "counter"
```

### Task 5: Validate Instrumentation Performance Impact

#### Performance Validation Script

**Location**: `.github/scripts/validate-instrumentation-performance.sh`

```bash
#!/bin/bash
set -e

echo "üìä Validating instrumentation performance impact..."

# Baseline measurements (before instrumentation)
BASELINE_FRONTEND_BUILD=120  # seconds
BASELINE_BACKEND_BUILD=180   # seconds
BASELINE_TESTS=60            # seconds

# Function to measure workflow duration
measure_workflow() {
  local workflow_file="$1"
  local run_id="$2"

  gh run view "$run_id" --json jobs --jq '.jobs[] | .name + ": " + (.completedAt - .startedAt | tonumber / 1000000000 | floor | tostring) + "s"'
}

# Trigger test workflow runs
echo "üöÄ Triggering instrumented workflow runs..."

# Frontend CI
FRONTEND_RUN=$(gh workflow run frontend-ci.yml --ref $(git branch --show-current) --json id --jq '.id')
sleep 5
gh run watch "$FRONTEND_RUN" --exit-status || true

# Backend CI
BACKEND_RUN=$(gh workflow run backend-ci.yml --ref $(git branch --show-current) --json id --jq '.id')
sleep 5
gh run watch "$BACKEND_RUN" --exit-status || true

# Measure durations
echo "üìè Measuring instrumented workflow durations..."

FRONTEND_DURATION=$(measure_workflow "frontend-ci.yml" "$FRONTEND_RUN" | grep "Build:" | grep -oP '\d+')
BACKEND_DURATION=$(measure_workflow "backend-ci.yml" "$BACKEND_RUN" | grep "Package:" | grep -oP '\d+')

# Calculate overhead
FRONTEND_OVERHEAD=$(echo "scale=2; (($FRONTEND_DURATION - $BASELINE_FRONTEND_BUILD) / $BASELINE_FRONTEND_BUILD) * 100" | bc)
BACKEND_OVERHEAD=$(echo "scale=2; (($BACKEND_DURATION - $BASELINE_BACKEND_BUILD) / $BASELINE_BACKEND_BUILD) * 100" | bc)

echo ""
echo "## Performance Impact Results"
echo ""
echo "| Workflow | Baseline | Instrumented | Overhead |"
echo "|----------|----------|--------------|----------|"
echo "| Frontend Build | ${BASELINE_FRONTEND_BUILD}s | ${FRONTEND_DURATION}s | ${FRONTEND_OVERHEAD}% |"
echo "| Backend Build | ${BASELINE_BACKEND_BUILD}s | ${BACKEND_DURATION}s | ${BACKEND_OVERHEAD}% |"
echo ""

# Validate overhead < 1%
if (( $(echo "$FRONTEND_OVERHEAD < 1.0" | bc -l) )) && (( $(echo "$BACKEND_OVERHEAD < 1.0" | bc -l) )); then
  echo "‚úÖ Performance impact validation PASSED (overhead < 1%)"
  exit 0
else
  echo "‚ùå Performance impact validation FAILED (overhead >= 1%)"
  echo "   Frontend overhead: ${FRONTEND_OVERHEAD}%"
  echo "   Backend overhead: ${BACKEND_OVERHEAD}%"
  exit 1
fi
```

Make executable:
```bash
chmod +x .github/scripts/validate-instrumentation-performance.sh
```

#### Performance Documentation

**Location**: `.github/workflows/PERFORMANCE_METRICS.md`

```markdown
# Workflow Instrumentation Performance Metrics

## Baseline Measurements (Pre-Instrumentation)

| Workflow | Job | Duration |
|----------|-----|----------|
| frontend-ci | Lint | 15s |
| frontend-ci | Type Check | 20s |
| frontend-ci | Test | 45s |
| frontend-ci | Build | 40s |
| frontend-ci | **Total** | **120s** |
| backend-ci | Compile | 60s |
| backend-ci | Unit Test | 45s |
| backend-ci | Integration Test | 50s |
| backend-ci | Package | 25s |
| backend-ci | **Total** | **180s** |

## Instrumented Measurements

To be measured after implementation...

## Performance Targets

- **Overhead Target**: <1% per workflow
- **Max Acceptable Overhead**: 2%
- **otel-cli Installation**: ~2-3 seconds (one-time per job)
- **Span Creation**: <100ms per span
- **Metric Export**: Async, non-blocking

## Optimization Strategies

If overhead exceeds 1%:

1. **Reduce Span Frequency**: Only create spans for major steps
2. **Batch Metrics**: Aggregate metrics and export in batches
3. **Async Export**: Ensure all telemetry exports are non-blocking
4. **Conditional Instrumentation**: Only instrument on main/develop branches
```

## Metrics Collected

### Workflow-Level Metrics

| Metric Name | Type | Description |
|-------------|------|-------------|
| `workflow.started` | counter | Workflow execution started |
| `workflow.completed` | counter | Workflow execution completed |
| `workflow.errors` | gauge | Number of errors in workflow |
| `validation.frontend.success` | counter | Frontend validation succeeded |
| `validation.frontend.failure` | counter | Frontend validation failed |
| `validation.backend.success` | counter | Backend validation succeeded |
| `validation.backend.failure` | counter | Backend validation failed |
| `validation.security.success` | counter | Security validation succeeded |
| `validation.security.failure` | counter | Security validation failed |

### Job-Level Metrics

| Metric Name | Type | Description |
|-------------|------|-------------|
| `frontend.lint.duration_seconds` | gauge | Lint job duration |
| `frontend.lint.success/failure` | counter | Lint job outcome |
| `frontend.typecheck.duration_seconds` | gauge | Type check duration |
| `frontend.test.coverage_percent` | gauge | Test coverage percentage |
| `frontend.test.passed` | counter | Tests passed count |
| `frontend.test.failed` | counter | Tests failed count |
| `frontend.build.bundle_size_bytes` | gauge | Production bundle size |
| `backend.compile.duration_seconds` | gauge | Compilation duration |
| `backend.test.run` | counter | Tests executed |
| `backend.test.failed` | counter | Tests failed |
| `backend.package.artifact_size_bytes` | gauge | JAR artifact size |
| `security.dependency.vulnerabilities` | gauge | Security vulnerabilities found |
| `lighthouse.performance_score` | gauge | Lighthouse performance score (0-100) |
| `lighthouse.accessibility_score` | gauge | Lighthouse accessibility score (0-100) |

## Spans Created

### Workflow Spans

- `workflow.validation-orchestrator` - Root span for entire orchestration
- `workflow.frontend-ci` - Frontend CI workflow execution
- `workflow.backend-ci` - Backend CI workflow execution

### Step Spans

- `workflow-started` - Workflow initialization
- `frontend-validation` - Frontend validation stage
- `backend-validation` - Backend validation stage
- `security-validation` - Security validation stage
- `validation-complete` - Validation completion
- `npm-ci` - npm dependency installation
- `eslint-check` - ESLint execution
- `typescript-check` - TypeScript compilation check
- `vitest-run` - Vitest test execution
- `vite-build` - Vite production build
- `mvn-compile` - Maven compilation
- `mvn-test` - Maven test execution
- `mvn-package` - Maven packaging

## Grafana Dashboard Queries

### Workflow Success Rate

```promql
sum(rate(workflow_completed{status="success"}[5m]))
/
sum(rate(workflow_completed[5m]))
```

### P95 Build Duration

```promql
histogram_quantile(0.95,
  sum(rate(frontend_build_duration_seconds_bucket[5m])) by (le)
)
```

### Test Failure Rate

```promql
sum(rate(frontend_test_failed[5m]))
/
(sum(rate(frontend_test_passed[5m])) + sum(rate(frontend_test_failed[5m])))
```

## Testing Instructions

1. **Test otel-cli installation**:
   ```bash
   # In any workflow job
   otel-cli --version
   ```

2. **Test span creation**:
   ```bash
   source $OTEL_HELPERS_PATH
   otel_span_step "test-span" "success"
   ```

3. **Test metric export**:
   ```bash
   source $OTEL_HELPERS_PATH
   otel_metric "test.metric" 42 "gauge"
   ```

4. **Verify traces in Grafana**:
   - Navigate to Grafana ‚Üí Explore ‚Üí Tempo
   - Search by trace ID from workflow logs
   - Verify complete span hierarchy

5. **Verify metrics in Prometheus**:
   ```bash
   curl http://prometheus:9090/api/v1/query?query=workflow_started
   ```

## Troubleshooting

### Spans not appearing

1. Check otel-cli installation:
   ```bash
   which otel-cli
   otel-cli --version
   ```

2. Verify OTel Collector endpoint:
   ```bash
   curl -v http://otel-collector.observability.svc.cluster.local:4317
   ```

3. Check environment variables:
   ```bash
   echo $OTEL_EXPORTER_OTLP_ENDPOINT
   echo $OTEL_SERVICE_NAME
   ```

### Metrics not exported

1. Verify helper functions are loaded:
   ```bash
   type otel_metric
   ```

2. Test manual metric:
   ```bash
   otel-cli counter add --name test.counter --value 1 --endpoint $OTEL_EXPORTER_OTLP_ENDPOINT
   ```

### High overhead

1. Reduce span creation frequency
2. Use conditional instrumentation:
   ```bash
   if [[ "$GITHUB_REF" == "refs/heads/main" ]]; then
     source $OTEL_HELPERS_PATH
     otel_exec "command" npm run build
   else
     npm run build
   fi
   ```

## Next Steps

After completing instrumentation:

1. ‚úÖ Verify all workflows are instrumented
2. ‚úÖ Run performance validation script
3. ‚úÖ Create Grafana dashboards (Story 7)
4. ‚úÖ Setup alerting rules (Story 8)
5. ‚úÖ Document runbooks (Story 12)
